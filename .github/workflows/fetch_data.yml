# Name of the automated workflow
name: Fetch Ocean Data

# Controls when the action runs
on:
  workflow_dispatch:
  schedule:
    - cron: '0 5 * * *'

# Defines the jobs that will run
jobs:
  # First job: Fetch data from Global Fishing Watch (Working)
  fetch-gfw-data:
    runs-on: ubuntu-latest
    steps:
      # ... (GFW steps remain the same, no changes needed here) ...
      - name: Check out repo
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      - name: Install dependencies
        run: pip install requests
      - name: Create GFW Python script
        run: |
          cat > gfw_script.py <<EOF
          import requests
          import os
          import json
          from datetime import datetime, date, timedelta
          print("--- Starting GFW Data Fetch using Events API v3 ---")
          api_key = os.getenv('GFW_API_KEY')
          end_date = datetime.utcnow().date() - timedelta(days=3)
          start_date = end_date - timedelta(days=3)
          start_date_str = start_date.strftime('%Y-%m-%d')
          end_date_str = end_date.strftime('%Y-%m-%d')
          url = "https://gateway.api.globalfishingwatch.org/v3/events"
          headers = { 'Authorization': f"Bearer {api_key}" }
          params = {
              'datasets[0]': 'public-global-fishing-events:latest',
              'types[0]': 'FISHING',
              'start-date': start_date_str,
              'end-date': end_date_str,
              'limit': 10000,
              'offset': 0
          }
          try:
              response = requests.get(url, headers=headers, params=params)
              response.raise_for_status()
              with open("fishing_events.geojson", "w") as f:
                  f.write(response.text)
              data = response.json()
              event_count = data.get('total', 0)
              print(f"Successfully saved {event_count} events to fishing_events.geojson")
          except requests.exceptions.HTTPError as e:
              print(f"!!! ERROR: Request Failed. Status Code: {e.response.status_code}")
              print(f"--- Server Response Body ---\n{e.response.text}\n--------------------------")
              raise e
          print("--- GFW Data Fetch Complete ---")
          EOF
      - name: Run GFW script
        env:
          GFW_API_KEY: ${{ secrets.GFW_API_KEY }}
        run: python gfw_script.py
      - name: Commit and push if it changed
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "Automated update of GFW fishing events"
          file_pattern: "fishing_events.geojson"

  # Second job: Fetch data from the ArcGIS Marine Species layer (with Pagination)
  fetch-biodiversity-data:
    runs-on: ubuntu-latest
    steps:
      - name: Check out repo
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      - name: Install dependencies
        run: pip install requests
      - name: Create Biodiversity Python script
        run: |
          cat > biodiversity_script.py <<EOF
          import requests
          import json

          print("--- Starting Paginated Biodiversity Data Fetch ---")
          
          base_url = "https://services9.arcgis.com/IkktFdUAcY3WrH25/arcgis/rest/services/Global_Marine_Species_Patterns_(55km)/FeatureServer/0/query"
          params = {
              'where': 'Rich_all >= 1',
              'outFields': 'Rich_all',
              'outSR': '4326',
              'f': 'geojson',
              'resultOffset': 0,
              'resultRecordCount': 2000 # Max records per request
          }
          
          all_features = []
          
          while True:
              print(f"Fetching features starting at offset {params['resultOffset']}...")
              response = requests.get(base_url, params=params)
              response.raise_for_status()
              data = response.json()
              
              features = data.get('features', [])
              if not features:
                  print("No more features returned, fetch complete.")
                  break
              
              all_features.extend(features)
              print(f"Fetched {len(features)} features. Total so far: {len(all_features)}")
              
              # Check if the transfer limit was exceeded to decide if we need to continue
              if not data.get('properties', {}).get('exceededTransferLimit', False):
                  print("Transfer limit not exceeded, fetch complete.")
                  break
              
              # If we need to continue, update the offset for the next page
              params['resultOffset'] += len(features)

          # Reconstruct the final GeoJSON object with all features
          final_geojson = {
              "type": "FeatureCollection",
              "features": all_features
          }
          
          with open("biodiversity_richness.geojson", "w") as f:
              json.dump(final_geojson, f)
          
          print(f"Successfully fetched a total of {len(all_features)} biodiversity features and saved to biodiversity_richness.geojson")
          EOF
      - name: Run Biodiversity script
        run: python biodiversity_script.py
      - name: Commit and push if it changed
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "Automated update of marine biodiversity data"
          file_pattern: "biodiversity_richness.geojson"
